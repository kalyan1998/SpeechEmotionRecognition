{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Saved.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3r2QGj5UXwSENNJC/aaBc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"K73TlSGHe1Bp"},"source":["# berlin speech database\n","import os\n","from pydub import AudioSegment\n","\n","su = 0\n","for j, fn in enumerate(os.listdir('wav')):\n","  audio = AudioSegment.from_wav('wav/'+fn)\n","  if len(audio)>5000:\n","    trimmed_audio = audio[:5000]\n","    trimmed_audio.export(\"AllSpeechFiles/berlin\"+fn,format = \"wav\")\n","  else:\n","    audio.export(\"AllSpeechFiles/berlin\"+fn,format = \"wav\")\n","  su = su+1\n","\n","# ravdess speech database\n","for i in range(1,25):\n","  if i<10:\n","    directory = \"Actor_0\"+str(i)\n","  else:\n","    directory = \"Actor_\"+str(i)\n","  for j, fn in enumerate(os.listdir(directory)):\n","    audio = AudioSegment.from_wav(directory+'/'+fn)\n","    if len(audio)>5000:\n","      trimmed_audio = audio[:5000]\n","      trimmed_audio.export(\"AllSpeechFiles/ravdess\"+fn,format = \"wav\")\n","    else:\n","      audio.export(\"AllSpeechFiles/ravdess\"+fn,format = \"wav\")\n","    su = su+1\n","\n","# TESS toronto database\n","for subdir, dirs, files in os.walk(\"TESS Toronto emotional speech set data\"):\n","  for file in files:\n","    if os.path.join(subdir, file) == \"TESS Toronto emotional speech set data/YAF_fear/YAF_neat_fear.wav\":\n","      continue\n","    if os.path.join(subdir, file) == \"TESS Toronto emotional speech set data/YAF_angry/YAF_germ_angry.wav\":\n","      continue\n","    audio = AudioSegment.from_wav(os.path.join(subdir, file))\n","    if len(audio)>5000:\n","      trimmed_audio = audio[:5000]\n","      trimmed_audio.export(\"AllSpeechFiles/TESS\"+file,format = \"wav\")\n","    else:\n","      audio.export(\"AllSpeechFiles/TESS\"+file,format = \"wav\")\n","    su = su+1\n","\n","# cremad database\n","for j, fn in enumerate(os.listdir('AudioWAV')):\n","  audio = AudioSegment.from_wav('AudioWAV/'+fn)\n","  if len(audio)>5000:\n","    trimmed_audio = audio[:5000]\n","    trimmed_audio.export(\"AllSpeechFiles/cremad\"+fn,format = \"wav\")\n","  else:\n","    audio.export(\"AllSpeechFiles/cremad\"+fn,format = \"wav\")\n","  su = su+1\n","\n","# Savee database\n","for subdir, dirs, files in os.walk(\"AudioData\"):\n","  for dir in dirs:\n","    for j, fn in enumerate(os.listdir('AudioData/'+dir)):\n","      audio = AudioSegment.from_wav('AudioData/'+dir+'/'+fn)\n","      if len(audio)>5000:\n","        trimmed_audio = audio[:5000]\n","        trimmed_audio.export(\"AllSpeechFiles/savee\"+dir+fn,format = \"wav\")\n","      else:\n","        audio.export(\"AllSpeechFiles/savee\"+dir+fn,format = \"wav\")\n","      su = su+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKMiyaGHe8n1"},"source":["import numpy\n","import scipy.io.wavfile\n","from scipy.fftpack import dct\n","import os\n","import matplotlib.pyplot as plt\n","import librosa\n","import librosa.display\n","\n","for j, fn in enumerate(os.listdir('AllSpeechFiles')):\n","  audio = AudioSegment.from_wav('AllSpeechFiles/'+fn)\n","  sample_rate, signal = scipy.io.wavfile.read('AllSpeechFiles/'+fn)\n","  pre_emphasis = 0.97\n","  frame_size = 0.025\n","  frame_stride = 0.01\n","  NFFT = 512\n","  nfilt = 40\n","  if len(audio) == 5000:\n","    sample_rate = 16000\n","  else:\n","    a = len(audio)//1000\n","    sample_rate = 80000//a\n","  emphasized_signal = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n","  frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n","  signal_length = len(emphasized_signal)\n","  frame_length = int(round(frame_length))\n","  frame_step = int(round(frame_step))\n","  num_frames = int(numpy.ceil(float(numpy.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n","\n","  pad_signal_length = num_frames * frame_step + frame_length\n","  z = numpy.zeros((pad_signal_length - signal_length))\n","  pad_signal = numpy.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n","\n","  indices = numpy.tile(numpy.arange(0, frame_length), (num_frames, 1)) + numpy.tile(numpy.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n","  frames = pad_signal[indices.astype(numpy.int32, copy=False)]\n","  frames *= numpy.hamming(frame_length)\n","  mag_frames = numpy.absolute(numpy.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n","  pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n","  low_freq_mel = 0\n","  high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n","  mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n","  hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n","  bin = numpy.floor((NFFT + 1) * hz_points / sample_rate)\n","\n","  fbank = numpy.zeros((nfilt, int(numpy.floor(NFFT / 2 + 1))))\n","  for m in range(1, nfilt + 1):\n","      f_m_minus = int(bin[m - 1])   # left\n","      f_m = int(bin[m])             # center\n","      f_m_plus = int(bin[m + 1])    # right\n","\n","      for k in range(f_m_minus, f_m):\n","          fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n","      for k in range(f_m, f_m_plus):\n","          fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n","  filter_banks = numpy.dot(pow_frames, fbank.T)\n","  filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)  # Numerical Stability\n","  filter_banks = 20 * numpy.log10(filter_banks)  # dB\n","  #filter_banks -= (numpy.mean(filter_banks, axis=0) + 1e-8)\n","  sgram_mag, _ = librosa.magphase(filter_banks)\n","  mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate)\n","  librosa.display.specshow(mel_scale_sgram)\n","  plt.colorbar(format='%+2.0f dB')\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuQLNvC3fBqE"},"source":["# emo-db\n","categories = {\n","    \"W\": 5,\n","    \"L\": 4,\n","    \"E\": 7,\n","    \"A\": 6,\n","    \"F\": 3,\n","    \"T\": 4,\n","    \"N\": 1\n","}\n","\n","# ravdess\n","emotions={\n","  '01':1,\n","  '03':3,\n","  '04':4,\n","  '05':5,\n","  '06':6,\n","  '07':7,\n","  '08':8\n","}\n","\n","for file in glob.glob(\"Actor_*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  if file_name.split(\"-\")[2] == '02':\n","    continue\n","  emotion=emotions[file_name.split(\"-\")[2]]\n","  x.append(file)\n","  y.append(emotion)\n","\n","#cremad\n","emotions={\n","  'NEU':1,\n","  'HAP':3,\n","  'SAD':4,\n","  'ANG':5,\n","  'FEA':6,\n","  'DIS':7,\n","}\n","\n","for file in glob.glob(\"AudioWAV/*.wav\"):\n","  file_name=os.path.basename(file)\n","  emotion=emotions[file_name.split(\"_\")[2]]\n","  x.append(file)\n","  y.append(emotion)\n","\n","#savee\n","emotions={\n","  'n':1,\n","  'h':3,\n","  'sa':4,\n","  'a':5,\n","  'f':6,\n","  'd':7,\n","  'su':8\n","}\n","l = ['n','h','a','f','d']\n","for file in glob.glob(\"AudioData/*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  if file_name[0] in l:\n","    emotion=emotions[file_name[0]]\n","  else:\n","    emotion = emotions[file_name[0]+file_name[1]]\n","  x.append(file)\n","  y.append(emotion)\n","\n","# tess\n","emotions={\n","  'neutral':1,\n","  'happy':3,\n","  'sad':4,\n","  'angry':5,\n","  'fear':6,\n","  'disgust':7,\n","  'ps':8\n","}\n","\n","for file in glob.glob(\"TESS Toronto emotional speech set data/*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  emotion=emotions[file_name.split(\"_\")[2][:-4]]\n","  x.append(file)\n","  y.append(emotion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoZxpROClkdj"},"source":["from fastai.vision.all import *\n","from fastaudio.core.all import *\n","from fastaudio.augment.all import *\n","\n","class StatsRecorder:\n","    def __init__(self, red_dims=(0,2,3)):\n","        \"\"\"Accumulates normalization statistics across mini-batches.\n","        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n","        \"\"\"\n","        self.red_dims = red_dims # which mini-batch dimensions to average over\n","        self.nobservations = 0   # running number of observations\n","\n","    def update(self, data):\n","        \"\"\"\n","        data: ndarray, shape (nobservations, ndimensions)\n","        \"\"\"\n","        # initialize stats and dimensions on first batch\n","        if self.nobservations == 0:\n","            self.mean = data.mean(dim=self.red_dims, keepdim=True)\n","            self.std  = data.std (dim=self.red_dims,keepdim=True)\n","            self.nobservations = data.shape[0]\n","            self.ndimensions   = data.shape[1]\n","        else:\n","            if data.shape[1] != self.ndimensions:\n","                raise ValueError('Data dims do not match previous observations.')\n","            \n","            # find mean of new mini batch\n","            newmean = data.mean(dim=self.red_dims, keepdim=True)\n","            newstd  = data.std(dim=self.red_dims, keepdim=True)\n","            \n","            # update number of observations\n","            m = self.nobservations * 1.0\n","            n = data.shape[0]\n","\n","            # update running statistics\n","            tmp = self.mean\n","            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n","            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n","                        m*n/(m+n)**2 * (tmp - newmean)**2\n","            self.std  = torch.sqrt(self.std)\n","                                 \n","            # update total number of seen samples\n","            self.nobservations += n\n","\n","class AudioNormalize(Transform):\n","    \"Normalizes a single `AudioTensor`.\"\n","    def encodes(self, x:AudioTensor): return (x-x.mean()) / x.std()\n","\n","def CrossValidationSplitter(col='fold', fold=1):\n","    \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n","    def _inner(o):\n","        assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n","        col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n","        valid_idx = (col_values == fold).values.astype('bool')\n","        return IndexSplitter(mask2idxs(valid_idx))(o)\n","    return _inner\n","\n","cfg = AudioConfig.BasicSpectrogram() # with default torchaudio parameters\n","audio2spec = AudioToSpec.from_cfg(cfg)\n","path = \"AllSpeechFiles/\"\n","auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n","                 get_x=ColReader(\"file_name\"), \n","                 item_tfms = [AudioNormalize],\n","                 batch_tfms = [audio2spec],\n","                 get_y=ColReader(\"emotion\"))\n","dbunch = auds.dataloaders(df, bs=64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZYPisvulnGf"},"source":["# create recorders\n","global_stats  = StatsRecorder()\n","channel_stats = StatsRecorder(red_dims=(0,1,3))\n","\n","# step through the training dataset\n","with torch.no_grad():\n","    for idx,(x,y) in enumerate(iter(dbunch.train)):\n","        # update normalization statistics\n","        global_stats.update(x)\n","        channel_stats.update(x)\n","    \n","# parse out both sets of stats\n","global_mean,global_std = global_stats.mean,global_stats.std\n","channel_mean,channel_std = channel_stats.mean,channel_stats.std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uas3wWSplsZ9"},"source":["import pydub \n","import numpy as np\n","from numpy import save\n","import audio2numpy as a2n\n","\n","def read(f, normalized=False):\n","    \"\"\"MP3 to numpy array\"\"\"\n","    a = pydub.AudioSegment.from_mp3(f)\n","    y = np.array(a.get_array_of_samples())\n","    if a.channels == 2:\n","        y = y.reshape((-1, 2))\n","    if normalized:\n","        return a.frame_rate, np.float32(y) / 2**15\n","    else:\n","        return a.frame_rate, y\n","\n","for i in range(len(x)):\n","  npym,sr=a2n.audio_from_file(x[i])\n","  name = x[i].split('/')[-1]\n","  name = name[:-4]\n","  save(\"npydirec/\"+name+'.npy', npym)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qzR-iAIWE_e"},"source":["# ravdess\n","emotions={\n","  '01':'neutral',\n","  '03':'Happy',\n","  '04':'Sad',\n","  '05':'Angry',\n","  '06':'Fear',\n","  '07':'Disgust',\n","  '08':'Surprised'\n","}\n","\n","for file in glob.glob(\"Actor_*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  if file_name.split(\"-\")[2] == '02':\n","    continue\n","  emotion=emotions[file_name.split(\"-\")[2]]\n","  if emotion == 'Surprised':\n","    continue\n","  x.append(file)\n","  y.append(emotion)\n","  #shutil.copy(file,'AllSpeechFiles/')\n","\n","#savee\n","emotions={\n","  'n':'Neutral',\n","  'h':'Happy',\n","  'sa':'Sad',\n","  'a':'Angry',\n","  'f':'Fear',\n","  'd':'Disgust',\n","  'su':'Surprised'\n","}\n","\n","l = ['n','h','a','f','d']\n","for file in glob.glob(\"AudioData/*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  if file_name[2] in l:\n","    emotion=emotions[file_name[2]]\n","  else:\n","    emotion = emotions[file_name[2]+file_name[3]]\n","  if emotion=='Surprised':\n","    continue\n","  x.append(file)\n","  y.append(emotion)\n","  #shutil.copy(file,'AllSpeechFiles/')\n","\n","# tess\n","emotions={\n","  'neutral':'Neutral',\n","  'happy':'Happy',\n","  'sad':'Sad',\n","  'angry':'Angry',\n","  'fear':'Fear',\n","  'disgust':'Disgust',\n","  'ps':'Surprised'\n","}\n","\n","for file in glob.glob(\"TESS Toronto emotional speech set data/*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  emotion=emotions[file_name.split(\"_\")[2][:-4]]\n","  if emotion=='Surprised':\n","    continue\n","  x.append(file)\n","  y.append(emotion)\n","\n","# ravdess\n","emotions={\n","  '01':'neutral',\n","  '03':'Happy',\n","  '04':'Sad',\n","  '05':'Angry',\n","  '06':'Fear',\n","  '07':'Disgust',\n","  '08':'Surprised'\n","}\n","\n","for file in glob.glob(\"Actor_*/*.wav\"):\n","  file_name=os.path.basename(file)\n","  if file_name.split(\"-\")[2] == '02':\n","    continue\n","  emotion=emotions[file_name.split(\"-\")[2]]\n","  if emotion == 'Surprised':\n","    continue\n","  x.append(file)\n","  y.append(emotion)\n","  #shutil.copy(file,'AllSpeechFiles/')"],"execution_count":null,"outputs":[]}]}